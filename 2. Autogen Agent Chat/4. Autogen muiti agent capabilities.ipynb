{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messages in Autogen v0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imaging messages as the way agent communicate - text our Friend. \n",
    "\n",
    "When we communicate with the agents -----> sending a message\n",
    "when it responds ---> it too sends a message\n",
    "\n",
    "TextMessage \n",
    "ImageMessage\n",
    "ToolMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lokesh/Autogen/venv/lib/python3.13/site-packages/autogen_ext/models/openai/_openai_client.py:439: UserWarning: Missing required field 'structured_output' in ModelInfo. This field will be required in a future version of AutoGen.\n",
      "  validate_model_info(self._model_info)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage,MultiModalMessage\n",
    "from autogen_core import Image as AGImage\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")\n",
    "model_client =  OpenAIChatCompletionClient(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"deepseek/deepseek-r1-0528:free\",\n",
    "    api_key = api_key,\n",
    "    model_info={\n",
    "        \"family\":'deepseek',\n",
    "        \"vision\" :True,\n",
    "        \"function_calling\":True,\n",
    "        \"json_output\": False\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest Type of Message - TextMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AssistantAgent(\n",
    "    name = \"text_agent\",\n",
    "    model_client=model_client,\n",
    "    system_message='You are a helpful assistant. answer question accurately'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the United States of America (USA) is **Washington, D.C.** (District of Columbia).  \n",
      "\n",
      "Key details:  \n",
      "- **Washington, D.C.** is a federal district, not part of any U.S. state.  \n",
      "- It houses critical government institutions like the White House, U.S. Capitol, and Supreme Court.  \n",
      "- The city was established in 1790 and named after George Washington.  \n",
      "\n",
      "If you have additional questions, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "async def test_text_messages():\n",
    "    text_msg = TextMessage(content = 'What is the capital of USA?', source='user')\n",
    "    result = await agent.run(task=text_msg)\n",
    "    print(result.messages[-1].content)\n",
    "\n",
    "await test_text_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Modal Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help! However, I currently **can't view or analyze images** directly. As a text-based model, I can only process and respond to written descriptions or requests.\n",
      "\n",
      "If you:\n",
      "- **Describe the image** to me in words,  \n",
      "- Ask a question **related to content shown in the image** (subject, symbolism, etc.), or  \n",
      "- Provide any **context** about what you see,  \n",
      "\n",
      "Iâ€™ll do my best to assist! Alternatively, you can also upload an image to image-recognition tools like **Google Lens**, **Bing Image Search**, or other dedicated platforms. ðŸ˜Š\n",
      "\n",
      "What would you like to know?\n"
     ]
    }
   ],
   "source": [
    "async def test_multi_modal():\n",
    "\n",
    "    response = requests.get('https://picsum.photos/id/237/200/300') # 23 for the image of folks\n",
    "    pil_image = Image.open(BytesIO(response.content))\n",
    "    ag_image = AGImage(pil_image)\n",
    "\n",
    "    multi_modal_msg = MultiModalMessage(\n",
    "        content = ['What is in the image?',ag_image],\n",
    "        source='user'\n",
    "    )\n",
    "\n",
    "    result = await agent.run(task=multi_modal_msg)\n",
    "    print(result.messages[-1].content)\n",
    "\n",
    "\n",
    "await test_multi_modal()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
